{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_CTC_Vietnamese_Recognition_25epochs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/huyhoang17/Colab_Temporary/blob/master/Training_CTC_Vietnamese_Recognition_25epochs.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "4qQkFHnrwixn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "c3ee36d8-a0ae-4a3f-e4e0-85d14adfdb15"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3SyRUanny2aR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "69761c22-05a3-475c-f232-d1c047001ec4"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!ls drive/My\\ Drive/Japanese_Recognition/datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anpr_ocr      model\t\t\t  vn_handwriting_data.zip\n",
            "anpr_ocr.zip  pp_vn_handwriting_data\t  wordlist_bi_clean.txt\n",
            "ETL1\t      pp_vn_handwriting_data.zip  wordlist_mono_clean.txt\n",
            "IAM_dataset   transcription.pk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "haVgOTr-Es6n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "import pickle\n",
        "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "# print(TPU_WORKER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-LnOdZA5fmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !rm -rf drive/My\\ Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L6_swBz9-O1L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !unzip drive/My\\ Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data.zip -d drive/My\\ Drive/Japanese_Recognition/datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BC83ooB3yija",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# zip_ref = zipfile.ZipFile('drive/My Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data.zip', 'r')\n",
        "# zip_ref.extractall('drive/My Drive/Japanese_Recognition/datasets')\n",
        "# zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vI9Au8IW6WCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21220496-0d9e-4fb5-91cf-90a430f3e55d"
      },
      "cell_type": "code",
      "source": [
        "!ls drive/My\\ Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data | wc -l "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "56AYZWUI7WGz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db39eab4-7f25-445f-8b7d-1d1e4cfcc69e"
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('drive/My Drive/Japanese_Recognition/datasets/transcription.pk', 'rb') as f:  # noqa\n",
        "    data = pickle.load(f)\n",
        "no_samples = len(data)\n",
        "print(no_samples)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c4IaycD10EFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pp_dataset = 'drive/My Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data'\n",
        "img_size = (1150, 32)\n",
        "no_epochs = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2JrkIt5oxnVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "46c1b01e-9cf5-4f37-cc8c-021c7e7538a5"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "print('Keras version:', keras.__version__)\n",
        "import os\n",
        "from os.path import join\n",
        "import json\n",
        "import random\n",
        "import itertools\n",
        "import re\n",
        "import datetime\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "import pylab\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Input, Dense, Activation\n",
        "from keras.layers import Reshape, Lambda\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing import image\n",
        "import keras.callbacks\n",
        "import cv2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 1.11.0-rc2\n",
            "Keras version: 2.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DxAH3MKjxVMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chars = '\\ !%\"#&\\'()*+,-./0123456789:;?AÁẢÀÃẠÂẤẨẦẪẬĂẮẲẰẴẶBCDĐEÉẺÈẼẸÊẾỂỀỄỆFGHIÍỈÌĨỊJKLMNOÓỎÒÕỌÔỐỔỒỖỘƠỚỞỜỠỢPQRSTUÚỦÙŨỤƯỨỬỪỮỰVWXYÝỶỲỸỴZaáảàãạâấẩầẫậăắẳằẵặbcdđeéẻèẽẹêếểềễệfghiíỉìĩịjklmnoóỏòõọôốổồỗộơớởờỡợpqrstuúủùũụưứửừữựvwxyýỷỳỹỵz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lyjY5oqx1jEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d14163f0-6b96-44f2-f197-b7b8ccc9b792"
      },
      "cell_type": "code",
      "source": [
        "len(chars)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "QNbh90B6sgcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "136180e9-b31c-4626-f81a-037bcd9ca2b6"
      },
      "cell_type": "code",
      "source": [
        "chars_ = []\n",
        "for char in chars:\n",
        "  chars_.append(char)\n",
        "print(chars_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\\\', ' ', '!', '%', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'Á', 'Ả', 'À', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ẩ', 'Ầ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ẳ', 'Ằ', 'Ẵ', 'Ặ', 'B', 'C', 'D', 'Đ', 'E', 'É', 'Ẻ', 'È', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ể', 'Ề', 'Ễ', 'Ệ', 'F', 'G', 'H', 'I', 'Í', 'Ỉ', 'Ì', 'Ĩ', 'Ị', 'J', 'K', 'L', 'M', 'N', 'O', 'Ó', 'Ỏ', 'Ò', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ổ', 'Ồ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ở', 'Ờ', 'Ỡ', 'Ợ', 'P', 'Q', 'R', 'S', 'T', 'U', 'Ú', 'Ủ', 'Ù', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ử', 'Ừ', 'Ữ', 'Ự', 'V', 'W', 'X', 'Y', 'Ý', 'Ỷ', 'Ỳ', 'Ỹ', 'Ỵ', 'Z', 'a', 'á', 'ả', 'à', 'ã', 'ạ', 'â', 'ấ', 'ẩ', 'ầ', 'ẫ', 'ậ', 'ă', 'ắ', 'ẳ', 'ằ', 'ẵ', 'ặ', 'b', 'c', 'd', 'đ', 'e', 'é', 'ẻ', 'è', 'ẽ', 'ẹ', 'ê', 'ế', 'ể', 'ề', 'ễ', 'ệ', 'f', 'g', 'h', 'i', 'í', 'ỉ', 'ì', 'ĩ', 'ị', 'j', 'k', 'l', 'm', 'n', 'o', 'ó', 'ỏ', 'ò', 'õ', 'ọ', 'ô', 'ố', 'ổ', 'ồ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ở', 'ờ', 'ỡ', 'ợ', 'p', 'q', 'r', 's', 't', 'u', 'ú', 'ủ', 'ù', 'ũ', 'ụ', 'ư', 'ứ', 'ử', 'ừ', 'ữ', 'ự', 'v', 'w', 'x', 'y', 'ý', 'ỷ', 'ỳ', 'ỹ', 'ỵ', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4azzwbUsso2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cce50ed-4638-4850-a2f7-fb3556a3a7f5"
      },
      "cell_type": "code",
      "source": [
        "len(chars_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "BCq_QEZM2BJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "def get_logger(name):\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    # formatter\n",
        "    fmt = logging.Formatter(\n",
        "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    # handler\n",
        "    handler = logging.StreamHandler()\n",
        "    handler.setLevel(logging.DEBUG)\n",
        "#     file_handler = logging.FileHandler(cf.LOGGING, mode='a')\n",
        "#     file_handler.setLevel(logging.DEBUG)\n",
        "    handler.setFormatter(fmt)\n",
        "#     file_handler.setFormatter(fmt)\n",
        "\n",
        "    # add handler to formatter\n",
        "    logger.addHandler(handler)\n",
        "#     logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "logger = get_logger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uusd0ma2xO6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def labels_to_text(letters, labels):\n",
        "    return ''.join(list(map(lambda x: letters[x] if x < len(letters) else \"\", labels)))  # noqa\n",
        "\n",
        "\n",
        "def text_to_labels(letters, text):\n",
        "    return list(map(lambda x: letters.index(x), text))\n",
        "\n",
        "\n",
        "def decode_batch(out):\n",
        "    ret = []\n",
        "    for j in range(out.shape[0]):\n",
        "        out_best = list(np.argmax(out[j, 2:], 1))\n",
        "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
        "        outstr = labels_to_text(out_best)\n",
        "        ret.append(outstr)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def decode_predict_ctc(out, chars, top_paths=1):\n",
        "    results = []\n",
        "    beam_width = 5\n",
        "    if beam_width < top_paths:\n",
        "        beam_width = top_paths\n",
        "    for i in range(top_paths):\n",
        "        lables = K.get_value(\n",
        "            K.ctc_decode(\n",
        "                out, input_length=np.ones(out.shape[0]) * out.shape[1],\n",
        "                greedy=False, beam_width=beam_width, top_paths=top_paths\n",
        "            )[0][i]\n",
        "        )[0]\n",
        "        text = labels_to_text(chars, lables)\n",
        "        results.append(text)\n",
        "    return results\n",
        "\n",
        "\n",
        "def predit_a_image(model_p, pimg, top_paths=1):\n",
        "    # c = np.expand_dims(a.T, axis=0)\n",
        "    net_out_value = model_p.predict(pimg)\n",
        "    top_pred_texts = decode_predict_ctc(net_out_value, top_paths)\n",
        "    return top_pred_texts\n",
        "\n",
        "\n",
        "def is_valid_str(letters, s):\n",
        "    for ch in s:\n",
        "        if ch not in letters:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DXE1cQ6AxZSS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    # the 2 is critical here since the first couple outputs of the RNN\n",
        "    # tend to be garbage:\n",
        "    y_pred = y_pred[:, 2:, :]\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5NkXRkTxfTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def CRNN_model():\n",
        "    act = 'relu'\n",
        "    input_data = Input(name='the_input', shape=img_size + (1, ), dtype='float32')\n",
        "    inner = Conv2D(16, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv1')(input_data)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max1')(\n",
        "        inner)\n",
        "    inner = Conv2D(32, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv2')(inner)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max2')(\n",
        "        inner)\n",
        "    inner = Conv2D(64, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv3')(input_data)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max3')(\n",
        "        inner)\n",
        "    inner = Conv2D(128, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv4')(inner)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max4')(\n",
        "        inner)\n",
        "    inner = Conv2D(256, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv5')(inner)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max5')(\n",
        "        inner)\n",
        "\n",
        "#     conv_to_rnn_dims = (1150 // (2 ** 2),\n",
        "#                         (32 // (2 ** 2)) * 16)\n",
        "    conv_to_rnn_dims = (256, 572)\n",
        "    print(conv_to_rnn_dims)\n",
        "#     import pdb; pdb.set_trace()\n",
        "    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n",
        "  \n",
        "    # cuts down input size going into RNN:\n",
        "    # TIME_DENSE_SIZE = 256\n",
        "    inner = Dense(256, activation=act, name='dense1')(inner)\n",
        "\n",
        "    gru_1 = GRU(256, return_sequences=True,\n",
        "                kernel_initializer='he_normal', name='gru1')(inner)\n",
        "    gru_1b = GRU(256, return_sequences=True, go_backwards=True,\n",
        "                 kernel_initializer='he_normal', name='gru1_b')(inner)\n",
        "    gru1_merged = add([gru_1, gru_1b])\n",
        "    gru_2 = GRU(256, return_sequences=True,\n",
        "                kernel_initializer='he_normal', name='gru2')(gru1_merged)\n",
        "    gru_2b = GRU(256, return_sequences=True, go_backwards=True,\n",
        "                 kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n",
        "\n",
        "    # transforms RNN output to character activations:\n",
        "    # no unique labels\n",
        "    inner = Dense(216, kernel_initializer='he_normal',\n",
        "                  name='dense2')(concatenate([gru_2, gru_2b]))\n",
        "    y_pred = Activation('softmax', name='softmax')(inner)\n",
        "\n",
        "    Model(inputs=input_data, outputs=y_pred).summary()\n",
        "\n",
        "    labels = Input(name='the_labels', shape=[256], dtype='float32')\n",
        "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "    # loss function\n",
        "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
        "        [y_pred, labels, input_length, label_length]\n",
        "    )\n",
        "\n",
        "    model = Model(inputs=[input_data, labels,\n",
        "                          input_length, label_length], outputs=loss_out)\n",
        "\n",
        "    y_func = K.function([input_data], [y_pred])\n",
        "\n",
        "    return model, y_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6qwD7COlxBoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TextSequenceGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Generates data for Keras\"\"\"\n",
        "\n",
        "    def __init__(self, samples, batch_size=16,\n",
        "                 img_size=img_size, max_text_len=160,\n",
        "                 downsample_factor=4, shuffle=True):\n",
        "        # train 95, test 5\n",
        "        imgs, gt_texts = [], []\n",
        "        for sample in samples:\n",
        "            img = list(sample.keys())[0]\n",
        "            fn_path = os.path.join(pp_dataset, img.split('/')[-1])\n",
        "            imgs.append(fn_path)\n",
        "            gt_texts.append(list(sample.values())[0])\n",
        "        self.imgs = imgs\n",
        "        self.gt_texts = gt_texts\n",
        "\n",
        "        self.max_text_len = max_text_len\n",
        "        self.chars = chars\n",
        "        self.blank_label = len(self.chars)\n",
        "        self.ids = range(len(self.imgs))\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.img_w, self.img_h = self.img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.downsample_factor = downsample_factor\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
        "        return int(np.floor(len(self.ids) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        indexes = self.indexes[index *\n",
        "                               self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        ids = [self.ids[k] for k in indexes]\n",
        "\n",
        "        for id_ in [1820, 5915]:\n",
        "            if id_ in ids:\n",
        "                ids.remove(id_)\n",
        "        X, y = self.__data_generation(ids)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        self.indexes = np.arange(len(self.ids))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, ids):\n",
        "        \"\"\"Generates data containing batch_size samples\"\"\"\n",
        "#         for i, id_ in enumerate(ids):\n",
        "#             img = cv2.imread(self.imgs[id_], cv2.IMREAD_GRAYSCALE)\n",
        "#             if img is None:\n",
        "#                 ids.remove(id_)\n",
        "#                 print(\"\\n==> Error id: \", id_)\n",
        "        size = len(ids)\n",
        "        \n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            X = np.ones([size, 1, self.img_w, self.img_h])\n",
        "        else:\n",
        "            X = np.ones([size, self.img_w, self.img_h, 1])\n",
        "        Y = np.ones([size, self.max_text_len])\n",
        "#         input_length = np.ones((size, 1), dtype=np.float32) * \\\n",
        "#             (self.img_w // self.downsample_factor - 2)\n",
        "        input_length = np.ones((size, 1), dtype=np.float32) * 254\n",
        "        label_length = np.zeros((size, 1), dtype=np.float32)\n",
        "\n",
        "        # Generate data\n",
        "        for i, id_ in enumerate(ids):\n",
        "            \n",
        "            img = cv2.imread(self.imgs[id_], cv2.IMREAD_GRAYSCALE)  # (h, w)\n",
        "            if img is None:\n",
        "                continue\n",
        "#             img = 255 - img  # bg: black, text: white\n",
        "            # bg: white, text: black\n",
        "            ratio = img.shape[0] / self.img_h\n",
        "            new_w = int(img.shape[1] / ratio) + 1\n",
        "            resized_image = cv2.resize(img, (new_w, self.img_h))  # (h, w)\n",
        "            img = cv2.copyMakeBorder(\n",
        "                resized_image, 0, 0, 0, self.img_w - resized_image.shape[1],\n",
        "                cv2.BORDER_CONSTANT, value=0\n",
        "            )  # (h, w)\n",
        "            img = img / 255  # (h, w)\n",
        "\n",
        "            if K.image_data_format() == 'channels_first':\n",
        "                img = np.expand_dims(img, 0)  # (1, h, w)\n",
        "                img = np.expand_dims((0, 2, 1))  # (1, w, h)\n",
        "            else:\n",
        "                img = np.expand_dims(img, -1)  # (h, w, 1)\n",
        "                img = img.transpose((1, 0, 2))  # (w, h, 1)\n",
        "\n",
        "            X[i] = img\n",
        "            text2label = text_to_labels(self.chars, self.gt_texts[id_])\n",
        "            Y[i] = text2label + \\\n",
        "                [self.blank_label for _ in range(\n",
        "                    self.max_text_len - len(text2label))]\n",
        "            label_length[i] = len(self.gt_texts[id_])\n",
        "\n",
        "        inputs = {\n",
        "            'the_input': X,\n",
        "            'the_labels': Y,\n",
        "            'input_length': input_length,\n",
        "            'label_length': label_length,\n",
        "        }\n",
        "        outputs = {'ctc': np.zeros([size])}\n",
        "\n",
        "        return (inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "di0kjTt0xKPF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(pretrained=False):\n",
        "\n",
        "    with open('drive/My Drive/Japanese_Recognition/datasets/transcription.pk', 'rb') as f:  # noqa\n",
        "        data = pickle.load(f)\n",
        "    no_samples = len(data)\n",
        "    no_train_set = int(no_samples * 0.95)\n",
        "    no_val_set = no_samples - no_train_set\n",
        "    logger.info(\"No train set: %d\", no_train_set)\n",
        "    logger.info(\"No val set: %d\", no_val_set)\n",
        "\n",
        "    train_set = TextSequenceGenerator(\n",
        "        data[:no_train_set],\n",
        "        img_size=img_size, max_text_len=256,\n",
        "        downsample_factor=4,\n",
        "        shuffle=True\n",
        "    )\n",
        "#     import pdb; pdb.set_trace()\n",
        "    test_set = TextSequenceGenerator(\n",
        "        data[no_train_set:],\n",
        "        img_size=img_size, max_text_len=256,\n",
        "        downsample_factor=4,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    model, y_func = CRNN_model()\n",
        "\n",
        "    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
        "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
        "\n",
        "    ckp = ModelCheckpoint(\n",
        "        \"drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_{}epochs.hdf5\".format(no_epochs), monitor='val_loss',\n",
        "        verbose=1, save_best_only=True, save_weights_only=True\n",
        "    )\n",
        "    earlystop = EarlyStopping(\n",
        "        monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'\n",
        "    )\n",
        "\n",
        "#     model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "#     model,\n",
        "#     strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "    \n",
        "    model.fit_generator(generator=train_set,\n",
        "                        steps_per_epoch=no_train_set // 16,\n",
        "                        epochs=no_epochs,\n",
        "                        validation_data=test_set,\n",
        "                        validation_steps=no_val_set // 16,\n",
        "                        callbacks=[ckp, earlystop])\n",
        "\n",
        "    return model, y_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNZZ66aHzf2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2278
        },
        "outputId": "9362976e-7ba2-4f66-87e9-20cc8793b5a2"
      },
      "cell_type": "code",
      "source": [
        "model, y_func = train()\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open('drive/My Drive/Japanese_Recognition/models/config_jps_{}epochs.json'.format(no_epochs), 'w') as f:\n",
        "    f.write(model_json)\n",
        "\n",
        "model.save_weights('drive/My Drive/Japanese_Recognition/models/best_model_CTC_jps_{}epochs.h5'.format(no_epochs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-10-01 05:54:28,808 - __main__ - INFO - No train set: 6931\n",
            "2018-10-01 05:54:28,810 - __main__ - INFO - No val set: 365\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(256, 572)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "the_input (InputLayer)          (None, 1150, 32, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv3 (Conv2D)                  (None, 1150, 32, 64) 640         the_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max3 (MaxPooling2D)             (None, 575, 16, 64)  0           conv3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4 (Conv2D)                  (None, 575, 16, 128) 73856       max3[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "max4 (MaxPooling2D)             (None, 287, 8, 128)  0           conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv5 (Conv2D)                  (None, 287, 8, 256)  295168      max4[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "max5 (MaxPooling2D)             (None, 143, 4, 256)  0           conv5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 256, 572)     0           max5[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense1 (Dense)                  (None, 256, 256)     146688      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru1 (GRU)                      (None, 256, 256)     393984      dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "gru1_b (GRU)                    (None, 256, 256)     393984      dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 256, 256)     0           gru1[0][0]                       \n",
            "                                                                 gru1_b[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "gru2 (GRU)                      (None, 256, 256)     393984      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru2_b (GRU)                    (None, 256, 256)     393984      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256, 512)     0           gru2[0][0]                       \n",
            "                                                                 gru2_b[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense2 (Dense)                  (None, 256, 216)     110808      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 256, 216)     0           dense2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,203,096\n",
            "Trainable params: 2,203,096\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Epoch 1/25\n",
            "433/433 [==============================] - 3099s 7s/step - loss: 230.4588 - val_loss: 208.9655\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 208.96552, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 2/25\n",
            "433/433 [==============================] - 964s 2s/step - loss: 203.8802 - val_loss: 205.9455\n",
            "\n",
            "Epoch 00002: val_loss improved from 208.96552 to 205.94549, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 3/25\n",
            "433/433 [==============================] - 966s 2s/step - loss: 199.3190 - val_loss: 198.5632\n",
            "\n",
            "Epoch 00003: val_loss improved from 205.94549 to 198.56322, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 4/25\n",
            "433/433 [==============================] - 961s 2s/step - loss: 189.3118 - val_loss: 183.9358\n",
            "\n",
            "Epoch 00004: val_loss improved from 198.56322 to 183.93580, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 5/25\n",
            "433/433 [==============================] - 967s 2s/step - loss: 169.5695 - val_loss: 162.8708\n",
            "\n",
            "Epoch 00005: val_loss improved from 183.93580 to 162.87077, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 6/25\n",
            "433/433 [==============================] - 972s 2s/step - loss: 146.4372 - val_loss: 143.2767\n",
            "\n",
            "Epoch 00006: val_loss improved from 162.87077 to 143.27674, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 7/25\n",
            "433/433 [==============================] - 967s 2s/step - loss: 125.1600 - val_loss: 126.5435\n",
            "\n",
            "Epoch 00007: val_loss improved from 143.27674 to 126.54355, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 8/25\n",
            "433/433 [==============================] - 967s 2s/step - loss: 107.2212 - val_loss: 112.3841\n",
            "\n",
            "Epoch 00008: val_loss improved from 126.54355 to 112.38406, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 9/25\n",
            "433/433 [==============================] - 967s 2s/step - loss: 93.0619 - val_loss: 104.5341\n",
            "\n",
            "Epoch 00009: val_loss improved from 112.38406 to 104.53405, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 10/25\n",
            "433/433 [==============================] - 970s 2s/step - loss: 81.5601 - val_loss: 98.3383\n",
            "\n",
            "Epoch 00010: val_loss improved from 104.53405 to 98.33828, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 11/25\n",
            "433/433 [==============================] - 967s 2s/step - loss: 72.2978 - val_loss: 93.4538\n",
            "\n",
            "Epoch 00011: val_loss improved from 98.33828 to 93.45376, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 12/25\n",
            "433/433 [==============================] - 965s 2s/step - loss: 64.6231 - val_loss: 91.7625\n",
            "\n",
            "Epoch 00012: val_loss improved from 93.45376 to 91.76250, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 13/25\n",
            "433/433 [==============================] - 964s 2s/step - loss: 58.0971 - val_loss: 91.6841\n",
            "\n",
            "Epoch 00013: val_loss improved from 91.76250 to 91.68406, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 14/25\n",
            "433/433 [==============================] - 963s 2s/step - loss: 52.4760 - val_loss: 88.4575\n",
            "\n",
            "Epoch 00014: val_loss improved from 91.68406 to 88.45754, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 15/25\n",
            "433/433 [==============================] - 964s 2s/step - loss: 47.5266 - val_loss: 88.2443\n",
            "\n",
            "Epoch 00015: val_loss improved from 88.45754 to 88.24426, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps_25epochs.hdf5\n",
            "Epoch 16/25\n",
            "433/433 [==============================] - 963s 2s/step - loss: 43.0692 - val_loss: 89.4305\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 88.24426\n",
            "Epoch 17/25\n",
            "433/433 [==============================] - 963s 2s/step - loss: 38.9591 - val_loss: 91.3146\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 88.24426\n",
            "Epoch 18/25\n",
            "433/433 [==============================] - 962s 2s/step - loss: 35.4944 - val_loss: 90.1916\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 88.24426\n",
            "Epoch 19/25\n",
            "433/433 [==============================] - 963s 2s/step - loss: 32.3926 - val_loss: 93.0026\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 88.24426\n",
            "Epoch 20/25\n",
            "433/433 [==============================] - 963s 2s/step - loss: 29.3753 - val_loss: 95.2967\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 88.24426\n",
            "Epoch 21/25\n",
            "433/433 [==============================] - 966s 2s/step - loss: 26.8272 - val_loss: 98.1853\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 88.24426\n",
            "Epoch 22/25\n",
            " 17/433 [>.............................] - ETA: 15:18 - loss: 23.0273"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gGf3GQX5nPl2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "with open('drive/My Drive/Japanese_Recognition/models/config_jps_25epochs.json') as f:\n",
        "    json_string = f.read()\n",
        "\n",
        "model = model_from_json(json_string)\n",
        "model.load_weights('drive/My Drive/Japanese_Recognition/models/best_model_CTC_jps_25epochs.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HuWh-Ddlgwru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_data = model.get_layer('the_input').output\n",
        "y_pred = model.get_layer('softmax').output\n",
        "model_p = Model(inputs=input_data, outputs=y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "af4rkQWihL8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0f09b86-d2cc-4aa8-e611-9c7fe44faac1"
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('drive/My Drive/Japanese_Recognition/datasets/transcription.pk', 'rb') as f:  # noqa\n",
        "    data = pickle.load(f)\n",
        "no_samples = len(data)\n",
        "no_train_set = int(no_samples * 0.95)\n",
        "no_val_set = no_samples - no_train_set\n",
        "test_set = TextSequenceGenerator(\n",
        "    data[no_train_set:],\n",
        "    img_size=img_size, max_text_len=256,\n",
        "    downsample_factor=4,\n",
        "    shuffle=False\n",
        ")\n",
        "texts = []\n",
        "for sample in data:\n",
        "  texts.append(list(sample.values())[0])\n",
        "print(max(map(len, texts)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l5-4ScvQiHXU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import scipy.misc\n",
        "# scipy.misc.imsave('outfile.jpg', image_array)\n",
        "def predict(index_batch, index_img):\n",
        "  samples = test_set[index_batch]\n",
        "  img = samples[0]['the_input'][index_img]\n",
        "  # plt.imshow(np.squeeze(img).T)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  print(img.shape)\n",
        "  \n",
        "  net_out_value = model_p.predict(img)\n",
        "  print(net_out_value.shape)\n",
        "  pred_texts = top_pred_texts = decode_predict_ctc(net_out_value, chars_)\n",
        "  print(pred_texts)\n",
        "  gt_texts = test_set[index_batch][0]['the_labels'][index_img]\n",
        "  gt_texts = labels_to_text(chars_, gt_texts.astype(int))\n",
        "  print(gt_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PYT-qMB78p_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1154
        },
        "outputId": "07cfa033-57a6-4c4d-c4be-c5e2d655a13a"
      },
      "cell_type": "code",
      "source": [
        "for i in range(16):\n",
        "    predict(1, i)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' lên cạnh việc nghệ sế KUNhY được tục hiêu kiện vô nưac biồn chểa']\n",
            "Bên cạnh việc các nghệ sĩ NVƠNN được tạo điều kiện về nước biểu diễn\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' phường quyễn, nhông\"- Tôi cũng trình tới tiện nời các nghận đở hạo sé']\n",
            "thường xuyên, chúng tôi cũng tính tới cả việc mời các nghệ sĩ hoặc các\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' vin đơng rên gó vớn Kan thuan ga cá cộc Ti qiốc đế dế mà cơ.']\n",
            "vận động viên gốc Việt tham gia các cuộc thi quốc tế dưới màu cờ\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' ưi \" n, ó Phế li ngàoy tịi Sêu ma tối, Nhóm gầi gháp nối sàyg']\n",
            "của VN, có thể là ngay tại Sea Games tới. Nhóm giải pháp cuối cùng\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' chú trụng sứí án, \" vẫn đề nhân tra : xăng dịm lông ton. là co béi']\n",
            "chú trọng tới các vấn đề nhân đạo và xây dựng lòng tin. Bà con sẽ\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' được tị điều kiên tã tà để ề Phầm quêc, Hhừ cn t trâng']\n",
            "được tạo điều kiện tối đa để về thăm quê, thờ cúng tổ tiên.\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' gaới tmắn hai con ó nhuâu th củny công dhung đà chền ở nớc để đao']\n",
            "Ngoài ra, nếu bà con có nhu cầu thờ cúng, xây chùa chiền ở nước sở tại,\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' nấi chủnh quền tổ tọi chếp nan, trong nước xẽ cơ biệp tộ cạn thế. thô']\n",
            "nếu được chính quyền sở tại chấp nhận, trong nước sẽ có hỗ trợ cụ thể. Thưa\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' côy. nuôn xãấ lể đước NH VNV bán khoán là tải cức chóp nhào của đi']\n",
            "ông, một vấn đề được nhiều NVƠNN băn khoăn là tư cách pháp nhân của bà\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' an bại nới àn tại để là con cò kãu cơ tú ơ lã ă mónh']\n",
            "con tại nước sở tại để bà con có thể cư trú và làm ăn một\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' cánh chính dúy. xấn đờ nôy sẽ đướa nua lân nm thươ ông \". Công, thao']\n",
            "cách chính đáng. Vấn đề này sẽ được quan tâm như thế nào thưa ông? - Vâng, theo\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' tính thền cải bộ Qnh Tui, 1ộ ngoại giao ã phải than mun o cốn']\n",
            "tinh thần của Bộ Chính trị, Bộ Ngoại giao sẽ phải tham mưu và kiến\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' phụ ụ thếi đã đào phán và lị tuốn với cài nớc nhống hậy dịnh']\n",
            "nghị cụ thể để đàm phán và ký kết với các nước những hiệp định\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' về 10 hấp cảnh phyếp lýt cúng công lâm VN, i dự chư cá bến bựng lonh']\n",
            "về tư cách pháp lý của công dân VN, ví dụ như các hiệp định lãnh\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' é không tạnh lệ nộ t gi.']\n",
            "sự, hiệp định hỗ trợ tư pháp\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' Ngài NK re mnh rống ở nác ngài bừ nhiều bàn cảnh khúc nhau,']\n",
            "Người VN ra sinh sống ở nước ngoài từ nhiều hoàn cảnh khác nhau, do\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tL2B38Hl4QMA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img = test_set[0][0]['the_input'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypRIHWQqApyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "ba9b7ee5-9980-4840-c7e3-ba3f1841fa49"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(np.squeeze(img).T)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdfaf1d2160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAAvCAYAAACi5cD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRORi/wEb4CGor3vS3wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdf97d32ac8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JygLx8_WAssT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.misc\n",
        "scipy.misc.imsave('drive/My Drive/Japanese_Recognition/images/test_image.png', np.squeeze(img).T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WK-7FW5ZBKBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4f7d79a0-ce12-4580-cd0d-15c75ebf0031"
      },
      "cell_type": "code",
      "source": [
        "np.squeeze(img)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "metadata": {
        "id": "pbLLt1bNBTyH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}