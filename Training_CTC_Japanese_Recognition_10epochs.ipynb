{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_CTC_Japanese_Recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/huyhoang17/Colab_Temporary/blob/master/Training_CTC_Japanese_Recognition_10epochs.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "4qQkFHnrwixn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "5d6c8327-a070-4ea2-f94d-cc172dadd1ec"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3SyRUanny2aR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b473d957-e0a0-4b44-e685-2fb549653b2a"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!ls drive/My\\ Drive/Japanese_Recognition/datasets"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anpr_ocr      model\t\t\t  wordlist_bi_clean.txt\n",
            "anpr_ocr.zip  pp_vn_handwriting_data\t  wordlist_mono_clean.txt\n",
            "ETL1\t      pp_vn_handwriting_data.zip\n",
            "IAM_dataset   transcription.pk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "haVgOTr-Es6n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "import pickle\n",
        "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "# print(TPU_WORKER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-LnOdZA5fmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !rm -rf drive/My\\ Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L6_swBz9-O1L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !unzip drive/My\\ Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data.zip -d drive/My\\ Drive/Japanese_Recognition/datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BC83ooB3yija",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# zip_ref = zipfile.ZipFile('drive/My Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data.zip', 'r')\n",
        "# zip_ref.extractall('drive/My Drive/Japanese_Recognition/datasets')\n",
        "# zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vI9Au8IW6WCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e5a3962-0894-45d1-a2ee-5b0d298eb9da"
      },
      "cell_type": "code",
      "source": [
        "!ls drive/My\\ Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data | wc -l "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7296\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "56AYZWUI7WGz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# with open('drive/My Drive/Japanese_Recognition/datasets/transcription.pk', 'rb') as f:  # noqa\n",
        "#     data = pickle.load(f)\n",
        "# no_samples = len(data)\n",
        "# print(no_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c4IaycD10EFk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pp_dataset = 'drive/My Drive/Japanese_Recognition/datasets/pp_vn_handwriting_data'\n",
        "img_size = (1150, 32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2JrkIt5oxnVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "e3eedbd2-cf4b-41e6-b39a-59551f918e31"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "print('Keras version:', keras.__version__)\n",
        "import os\n",
        "from os.path import join\n",
        "import json\n",
        "import random\n",
        "import itertools\n",
        "import re\n",
        "import datetime\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "import pylab\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers import Input, Dense, Activation\n",
        "from keras.layers import Reshape, Lambda\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers.recurrent import GRU\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing import image\n",
        "import keras.callbacks\n",
        "import cv2"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 1.11.0-rc2\n",
            "Keras version: 2.1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DxAH3MKjxVMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "chars = '\\ !%\"#&\\'()*+,-./0123456789:;?AÁẢÀÃẠÂẤẨẦẪẬĂẮẲẰẴẶBCDĐEÉẺÈẼẸÊẾỂỀỄỆFGHIÍỈÌĨỊJKLMNOÓỎÒÕỌÔỐỔỒỖỘƠỚỞỜỠỢPQRSTUÚỦÙŨỤƯỨỬỪỮỰVWXYÝỶỲỸỴZaáảàãạâấẩầẫậăắẳằẵặbcdđeéẻèẽẹêếểềễệfghiíỉìĩịjklmnoóỏòõọôốổồỗộơớởờỡợpqrstuúủùũụưứửừữựvwxyýỷỳỹỵz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lyjY5oqx1jEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e99332d6-7642-4ffa-ebd3-21374495260b"
      },
      "cell_type": "code",
      "source": [
        "len(chars)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "QNbh90B6sgcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c945a1bf-383c-4c88-8c07-ca76a9edd225"
      },
      "cell_type": "code",
      "source": [
        "chars_ = []\n",
        "for char in chars:\n",
        "  chars_.append(char)\n",
        "print(chars_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\\\', ' ', '!', '%', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'Á', 'Ả', 'À', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ẩ', 'Ầ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ẳ', 'Ằ', 'Ẵ', 'Ặ', 'B', 'C', 'D', 'Đ', 'E', 'É', 'Ẻ', 'È', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ể', 'Ề', 'Ễ', 'Ệ', 'F', 'G', 'H', 'I', 'Í', 'Ỉ', 'Ì', 'Ĩ', 'Ị', 'J', 'K', 'L', 'M', 'N', 'O', 'Ó', 'Ỏ', 'Ò', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ổ', 'Ồ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ở', 'Ờ', 'Ỡ', 'Ợ', 'P', 'Q', 'R', 'S', 'T', 'U', 'Ú', 'Ủ', 'Ù', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ử', 'Ừ', 'Ữ', 'Ự', 'V', 'W', 'X', 'Y', 'Ý', 'Ỷ', 'Ỳ', 'Ỹ', 'Ỵ', 'Z', 'a', 'á', 'ả', 'à', 'ã', 'ạ', 'â', 'ấ', 'ẩ', 'ầ', 'ẫ', 'ậ', 'ă', 'ắ', 'ẳ', 'ằ', 'ẵ', 'ặ', 'b', 'c', 'd', 'đ', 'e', 'é', 'ẻ', 'è', 'ẽ', 'ẹ', 'ê', 'ế', 'ể', 'ề', 'ễ', 'ệ', 'f', 'g', 'h', 'i', 'í', 'ỉ', 'ì', 'ĩ', 'ị', 'j', 'k', 'l', 'm', 'n', 'o', 'ó', 'ỏ', 'ò', 'õ', 'ọ', 'ô', 'ố', 'ổ', 'ồ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ở', 'ờ', 'ỡ', 'ợ', 'p', 'q', 'r', 's', 't', 'u', 'ú', 'ủ', 'ù', 'ũ', 'ụ', 'ư', 'ứ', 'ử', 'ừ', 'ữ', 'ự', 'v', 'w', 'x', 'y', 'ý', 'ỷ', 'ỳ', 'ỹ', 'ỵ', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4azzwbUsso2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb9ba99b-aa10-4e8a-b4ff-3a35722c025f"
      },
      "cell_type": "code",
      "source": [
        "len(chars_)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "215"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "BCq_QEZM2BJI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "def get_logger(name):\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    # formatter\n",
        "    fmt = logging.Formatter(\n",
        "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "\n",
        "    # handler\n",
        "    handler = logging.StreamHandler()\n",
        "    handler.setLevel(logging.DEBUG)\n",
        "#     file_handler = logging.FileHandler(cf.LOGGING, mode='a')\n",
        "#     file_handler.setLevel(logging.DEBUG)\n",
        "    handler.setFormatter(fmt)\n",
        "#     file_handler.setFormatter(fmt)\n",
        "\n",
        "    # add handler to formatter\n",
        "    logger.addHandler(handler)\n",
        "#     logger.addHandler(file_handler)\n",
        "\n",
        "    return logger\n",
        "logger = get_logger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uusd0ma2xO6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def labels_to_text(letters, labels):\n",
        "    return ''.join(list(map(lambda x: letters[x] if x < len(letters) else \"\", labels)))  # noqa\n",
        "\n",
        "\n",
        "def text_to_labels(letters, text):\n",
        "    return list(map(lambda x: letters.index(x), text))\n",
        "\n",
        "\n",
        "def decode_batch(out):\n",
        "    ret = []\n",
        "    for j in range(out.shape[0]):\n",
        "        out_best = list(np.argmax(out[j, 2:], 1))\n",
        "        out_best = [k for k, g in itertools.groupby(out_best)]\n",
        "        outstr = labels_to_text(out_best)\n",
        "        ret.append(outstr)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def decode_predict_ctc(out, chars, top_paths=1):\n",
        "    results = []\n",
        "    beam_width = 5\n",
        "    if beam_width < top_paths:\n",
        "        beam_width = top_paths\n",
        "    for i in range(top_paths):\n",
        "        lables = K.get_value(\n",
        "            K.ctc_decode(\n",
        "                out, input_length=np.ones(out.shape[0]) * out.shape[1],\n",
        "                greedy=False, beam_width=beam_width, top_paths=top_paths\n",
        "            )[0][i]\n",
        "        )[0]\n",
        "        text = labels_to_text(chars, lables)\n",
        "        results.append(text)\n",
        "    return results\n",
        "\n",
        "\n",
        "def predit_a_image(model_p, pimg, top_paths=1):\n",
        "    # c = np.expand_dims(a.T, axis=0)\n",
        "    net_out_value = model_p.predict(pimg)\n",
        "    top_pred_texts = decode_predict_ctc(net_out_value, top_paths)\n",
        "    return top_pred_texts\n",
        "\n",
        "\n",
        "def is_valid_str(letters, s):\n",
        "    for ch in s:\n",
        "        if ch not in letters:\n",
        "            return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DXE1cQ6AxZSS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        "    # the 2 is critical here since the first couple outputs of the RNN\n",
        "    # tend to be garbage:\n",
        "    y_pred = y_pred[:, 2:, :]\n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5NkXRkTxfTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def CRNN_model():\n",
        "    act = 'relu'\n",
        "    input_data = Input(name='the_input', shape=img_size + (1, ), dtype='float32')\n",
        "    inner = Conv2D(16, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv1')(input_data)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max1')(\n",
        "        inner)\n",
        "    inner = Conv2D(32, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv2')(inner)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max2')(\n",
        "        inner)\n",
        "    inner = Conv2D(64, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv3')(input_data)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max3')(\n",
        "        inner)\n",
        "    inner = Conv2D(128, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv4')(inner)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max4')(\n",
        "        inner)\n",
        "    inner = Conv2D(256, (3, 3), padding='same',\n",
        "                   activation=act, kernel_initializer='he_normal',\n",
        "                   name='conv5')(inner)\n",
        "    inner = MaxPooling2D(pool_size=(2, 2), name='max5')(\n",
        "        inner)\n",
        "\n",
        "#     conv_to_rnn_dims = (1150 // (2 ** 2),\n",
        "#                         (32 // (2 ** 2)) * 16)\n",
        "    conv_to_rnn_dims = (256, 572)\n",
        "    print(conv_to_rnn_dims)\n",
        "#     import pdb; pdb.set_trace()\n",
        "    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n",
        "  \n",
        "    # cuts down input size going into RNN:\n",
        "    # TIME_DENSE_SIZE = 256\n",
        "    inner = Dense(256, activation=act, name='dense1')(inner)\n",
        "\n",
        "    gru_1 = GRU(256, return_sequences=True,\n",
        "                kernel_initializer='he_normal', name='gru1')(inner)\n",
        "    gru_1b = GRU(256, return_sequences=True, go_backwards=True,\n",
        "                 kernel_initializer='he_normal', name='gru1_b')(inner)\n",
        "    gru1_merged = add([gru_1, gru_1b])\n",
        "    gru_2 = GRU(256, return_sequences=True,\n",
        "                kernel_initializer='he_normal', name='gru2')(gru1_merged)\n",
        "    gru_2b = GRU(256, return_sequences=True, go_backwards=True,\n",
        "                 kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n",
        "\n",
        "    # transforms RNN output to character activations:\n",
        "    # no unique labels\n",
        "    inner = Dense(216, kernel_initializer='he_normal',\n",
        "                  name='dense2')(concatenate([gru_2, gru_2b]))\n",
        "    y_pred = Activation('softmax', name='softmax')(inner)\n",
        "\n",
        "    Model(inputs=input_data, outputs=y_pred).summary()\n",
        "\n",
        "    labels = Input(name='the_labels', shape=[256], dtype='float32')\n",
        "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "    # loss function\n",
        "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
        "        [y_pred, labels, input_length, label_length]\n",
        "    )\n",
        "\n",
        "    model = Model(inputs=[input_data, labels,\n",
        "                          input_length, label_length], outputs=loss_out)\n",
        "\n",
        "    y_func = K.function([input_data], [y_pred])\n",
        "\n",
        "    return model, y_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6qwD7COlxBoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TextSequenceGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Generates data for Keras\"\"\"\n",
        "\n",
        "    def __init__(self, samples, batch_size=16,\n",
        "                 img_size=img_size, max_text_len=160,\n",
        "                 downsample_factor=4, shuffle=True):\n",
        "        # train 95, test 5\n",
        "        imgs, gt_texts = [], []\n",
        "        for sample in samples:\n",
        "            img = list(sample.keys())[0]\n",
        "            fn_path = os.path.join(pp_dataset, img.split('/')[-1])\n",
        "            imgs.append(fn_path)\n",
        "            gt_texts.append(list(sample.values())[0])\n",
        "        self.imgs = imgs\n",
        "        self.gt_texts = gt_texts\n",
        "\n",
        "        self.max_text_len = max_text_len\n",
        "        self.chars = chars\n",
        "        self.blank_label = len(self.chars)\n",
        "        self.ids = range(len(self.imgs))\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.img_w, self.img_h = self.img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.downsample_factor = downsample_factor\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
        "        return int(np.floor(len(self.ids) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\"\"\"\n",
        "        indexes = self.indexes[index *\n",
        "                               self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        ids = [self.ids[k] for k in indexes]\n",
        "\n",
        "        for id_ in [1820, 5915]:\n",
        "            if id_ in ids:\n",
        "                ids.remove(id_)\n",
        "        X, y = self.__data_generation(ids)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        self.indexes = np.arange(len(self.ids))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, ids):\n",
        "        \"\"\"Generates data containing batch_size samples\"\"\"\n",
        "#         for i, id_ in enumerate(ids):\n",
        "#             img = cv2.imread(self.imgs[id_], cv2.IMREAD_GRAYSCALE)\n",
        "#             if img is None:\n",
        "#                 ids.remove(id_)\n",
        "#                 print(\"\\n==> Error id: \", id_)\n",
        "        size = len(ids)\n",
        "        \n",
        "        if K.image_data_format() == 'channels_first':\n",
        "            X = np.ones([size, 1, self.img_w, self.img_h])\n",
        "        else:\n",
        "            X = np.ones([size, self.img_w, self.img_h, 1])\n",
        "        Y = np.ones([size, self.max_text_len])\n",
        "#         input_length = np.ones((size, 1), dtype=np.float32) * \\\n",
        "#             (self.img_w // self.downsample_factor - 2)\n",
        "        input_length = np.ones((size, 1), dtype=np.float32) * 254\n",
        "        label_length = np.zeros((size, 1), dtype=np.float32)\n",
        "\n",
        "        # Generate data\n",
        "        for i, id_ in enumerate(ids):\n",
        "            \n",
        "            img = cv2.imread(self.imgs[id_], cv2.IMREAD_GRAYSCALE)  # (h, w)\n",
        "            if img is None:\n",
        "                continue\n",
        "#             img = 255 - img  # bg: black, text: white\n",
        "            # bg: white, text: black\n",
        "            ratio = img.shape[0] / self.img_h\n",
        "            new_w = int(img.shape[1] / ratio) + 1\n",
        "            resized_image = cv2.resize(img, (new_w, self.img_h))  # (h, w)\n",
        "            img = cv2.copyMakeBorder(\n",
        "                resized_image, 0, 0, 0, self.img_w - resized_image.shape[1],\n",
        "                cv2.BORDER_CONSTANT, value=0\n",
        "            )  # (h, w)\n",
        "            img = img / 255  # (h, w)\n",
        "\n",
        "            if K.image_data_format() == 'channels_first':\n",
        "                img = np.expand_dims(img, 0)  # (1, h, w)\n",
        "                img = np.expand_dims((0, 2, 1))  # (1, w, h)\n",
        "            else:\n",
        "                img = np.expand_dims(img, -1)  # (h, w, 1)\n",
        "                img = img.transpose((1, 0, 2))  # (w, h, 1)\n",
        "\n",
        "            X[i] = img\n",
        "            text2label = text_to_labels(self.chars, self.gt_texts[id_])\n",
        "            Y[i] = text2label + \\\n",
        "                [self.blank_label for _ in range(\n",
        "                    self.max_text_len - len(text2label))]\n",
        "            label_length[i] = len(self.gt_texts[id_])\n",
        "\n",
        "        inputs = {\n",
        "            'the_input': X,\n",
        "            'the_labels': Y,\n",
        "            'input_length': input_length,\n",
        "            'label_length': label_length,\n",
        "        }\n",
        "        outputs = {'ctc': np.zeros([size])}\n",
        "\n",
        "        return (inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "di0kjTt0xKPF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(pretrained=False):\n",
        "\n",
        "    with open('drive/My Drive/Japanese_Recognition/datasets/transcription.pk', 'rb') as f:  # noqa\n",
        "        data = pickle.load(f)\n",
        "    no_samples = len(data)\n",
        "    no_train_set = int(no_samples * 0.95)\n",
        "    no_val_set = no_samples - no_train_set\n",
        "    logger.info(\"No train set: %d\", no_train_set)\n",
        "    logger.info(\"No val set: %d\", no_val_set)\n",
        "\n",
        "    train_set = TextSequenceGenerator(\n",
        "        data[:no_train_set],\n",
        "        img_size=img_size, max_text_len=256,\n",
        "        downsample_factor=4,\n",
        "        shuffle=True\n",
        "    )\n",
        "#     import pdb; pdb.set_trace()\n",
        "    test_set = TextSequenceGenerator(\n",
        "        data[no_train_set:],\n",
        "        img_size=img_size, max_text_len=256,\n",
        "        downsample_factor=4,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    model, y_func = CRNN_model()\n",
        "\n",
        "    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n",
        "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
        "\n",
        "    ckp = ModelCheckpoint(\n",
        "        \"drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\", monitor='val_loss',\n",
        "        verbose=1, save_best_only=True, save_weights_only=True\n",
        "    )\n",
        "    earlystop = EarlyStopping(\n",
        "        monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'\n",
        "    )\n",
        "\n",
        "#     model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "#     model,\n",
        "#     strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "#         tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "    \n",
        "    model.fit_generator(generator=train_set,\n",
        "                        steps_per_epoch=no_train_set // 16,\n",
        "                        epochs=10,\n",
        "                        validation_data=test_set,\n",
        "                        validation_steps=no_val_set // 16,\n",
        "                        callbacks=[ckp, earlystop])\n",
        "\n",
        "    return model, y_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNZZ66aHzf2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1583
        },
        "outputId": "80d0a60d-1e46-4e67-d867-3a6624e27107"
      },
      "cell_type": "code",
      "source": [
        "model, y_func = train()\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open('drive/My Drive/Japanese_Recognition/models/config_jps.json', 'w') as f:\n",
        "    f.write(model_json)\n",
        "\n",
        "model.save_weights('drive/My Drive/Japanese_Recognition/models/best_model_CTC_jps.h5')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2018-09-30 11:24:55,464 - __main__ - INFO - No train set: 6931\n",
            "2018-09-30 11:24:55,466 - __main__ - INFO - No val set: 365\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(256, 572)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "the_input (InputLayer)          (None, 1150, 32, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv3 (Conv2D)                  (None, 1150, 32, 64) 640         the_input[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max3 (MaxPooling2D)             (None, 575, 16, 64)  0           conv3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4 (Conv2D)                  (None, 575, 16, 128) 73856       max3[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "max4 (MaxPooling2D)             (None, 287, 8, 128)  0           conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv5 (Conv2D)                  (None, 287, 8, 256)  295168      max4[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "max5 (MaxPooling2D)             (None, 143, 4, 256)  0           conv5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 256, 572)     0           max5[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense1 (Dense)                  (None, 256, 256)     146688      reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru1 (GRU)                      (None, 256, 256)     393984      dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "gru1_b (GRU)                    (None, 256, 256)     393984      dense1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 256, 256)     0           gru1[0][0]                       \n",
            "                                                                 gru1_b[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "gru2 (GRU)                      (None, 256, 256)     393984      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru2_b (GRU)                    (None, 256, 256)     393984      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 256, 512)     0           gru2[0][0]                       \n",
            "                                                                 gru2_b[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense2 (Dense)                  (None, 256, 216)     110808      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "softmax (Activation)            (None, 256, 216)     0           dense2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,203,096\n",
            "Trainable params: 2,203,096\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
            "Epoch 1/10\n",
            "433/433 [==============================] - 1561s 4s/step - loss: 237.3214 - val_loss: 207.9095\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 207.90954, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n",
            "Epoch 2/10\n",
            "433/433 [==============================] - 988s 2s/step - loss: 203.6458 - val_loss: 203.7441\n",
            "\n",
            "Epoch 00002: val_loss improved from 207.90954 to 203.74413, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n",
            "Epoch 3/10\n",
            "433/433 [==============================] - 986s 2s/step - loss: 199.4905 - val_loss: 200.2314\n",
            "\n",
            "Epoch 00003: val_loss improved from 203.74413 to 200.23139, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n",
            "Epoch 4/10\n",
            "433/433 [==============================] - 987s 2s/step - loss: 194.6612 - val_loss: 195.0465\n",
            "\n",
            "Epoch 00004: val_loss improved from 200.23139 to 195.04649, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n",
            "Epoch 5/10\n",
            "433/433 [==============================] - 975s 2s/step - loss: 185.4192 - val_loss: 186.8454\n",
            "\n",
            "Epoch 00005: val_loss improved from 195.04649 to 186.84544, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n",
            "Epoch 6/10\n",
            "433/433 [==============================] - 981s 2s/step - loss: 168.2542 - val_loss: 164.0251\n",
            "\n",
            "Epoch 00006: val_loss improved from 186.84544 to 164.02506, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n",
            "Epoch 7/10\n",
            "433/433 [==============================] - 981s 2s/step - loss: 145.1871 - val_loss: 142.5686\n",
            "\n",
            "Epoch 00007: val_loss improved from 164.02506 to 142.56862, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n",
            "Epoch 8/10\n",
            "433/433 [==============================] - 976s 2s/step - loss: 124.7079 - val_loss: 125.9091\n",
            "\n",
            "Epoch 00008: val_loss improved from 142.56862 to 125.90906, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n",
            "Epoch 9/10\n",
            "433/433 [==============================] - 972s 2s/step - loss: 108.1700 - val_loss: 117.5038\n",
            "\n",
            "Epoch 00009: val_loss improved from 125.90906 to 117.50377, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n",
            "Epoch 10/10\n",
            "433/433 [==============================] - 970s 2s/step - loss: 95.1850 - val_loss: 107.2071\n",
            "\n",
            "Epoch 00010: val_loss improved from 117.50377 to 107.20708, saving model to drive/My Drive/Japanese_Recognition/models/model_cp_CTC_jps.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gGf3GQX5nPl2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "\n",
        "with open('drive/My Drive/Japanese_Recognition/models/config_jps.json') as f:\n",
        "    json_string = f.read()\n",
        "\n",
        "model = model_from_json(json_string)\n",
        "model.load_weights('drive/My Drive/Japanese_Recognition/models/best_model_CTC_jps.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HuWh-Ddlgwru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_data = model.get_layer('the_input').output\n",
        "y_pred = model.get_layer('softmax').output\n",
        "model_p = Model(inputs=input_data, outputs=y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "af4rkQWihL8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5fb613a-b4db-4ea5-9073-0fce6570622b"
      },
      "cell_type": "code",
      "source": [
        "with open('drive/My Drive/Japanese_Recognition/datasets/transcription.pk', 'rb') as f:  # noqa\n",
        "    data = pickle.load(f)\n",
        "no_samples = len(data)\n",
        "no_train_set = int(no_samples * 0.95)\n",
        "no_val_set = no_samples - no_train_set\n",
        "test_set = TextSequenceGenerator(\n",
        "    data[no_train_set:],\n",
        "    img_size=img_size, max_text_len=256,\n",
        "    downsample_factor=4,\n",
        "    shuffle=False\n",
        ")\n",
        "texts = []\n",
        "for sample in data:\n",
        "  texts.append(list(sample.values())[0])\n",
        "print(max(map(len, texts)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l5-4ScvQiHXU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import scipy.misc\n",
        "# scipy.misc.imsave('outfile.jpg', image_array)\n",
        "def predict(index_batch, index_img):\n",
        "  samples = test_set[index_batch]\n",
        "  img = samples[0]['the_input'][index_img]\n",
        "  # plt.imshow(np.squeeze(img).T)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  print(img.shape)\n",
        "  \n",
        "  net_out_value = model_p.predict(img)\n",
        "  print(net_out_value.shape)\n",
        "  pred_texts = top_pred_texts = decode_predict_ctc(net_out_value, chars_)\n",
        "  print(pred_texts)\n",
        "  gt_texts = test_set[index_batch][0]['the_labels'][index_img]\n",
        "  gt_texts = labels_to_text(chars_, gt_texts.astype(int))\n",
        "  print(gt_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PYT-qMB78p_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1201
        },
        "outputId": "4e779f95-99cb-4dde-cf5d-04cb1de8acd2"
      },
      "cell_type": "code",
      "source": [
        "for i in range(16):\n",
        "    predict(1, i)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' têm cạnh vệc nị sế Nở chược bạo âu biện và nưa bàn ciển']\n",
            "Bên cạnh việc các nghệ sĩ NVƠNN được tạo điều kiện về nước biểu diễn\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' phờng quyên, nhông tan cang thnh ếi riệc mỗi củn nghệ hại sẽ']\n",
            "thường xuyên, chúng tôi cũng tính tới cả việc mời các nghệ sĩ hoặc các\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' tện đang Vên gả kợn han kha gha các cột tha quác tó dú mào dó']\n",
            "vận động viên gốc Việt tham gia các cuộc thi quốc tế dưới màu cờ\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' canh N), kh la gày tạ vên hn tói. Thón gả phýp cấc y']\n",
            "của VN, có thể là ngay tại Sea Games tới. Nhóm giải pháp cuối cùng\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' chú huạng tó có vốo đề như trạn i ông dhạn lông bạn. a on tó']\n",
            "chú trọng tới các vấn đề nhân đạo và xây dựng lòng tin. Bà con sẽ\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' tượch tụ tiều hiện bến ta đã về thển qua, k àng trên']\n",
            "được tạo điều kiện tối đa để về thăm quê, thờ cúng tổ tiên.\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' C ui rác bạ a ó nhều khư cang cang dang ê hiền ở cớc sở a']\n",
            "Ngoài ra, nếu bà con có nhu cầu thờ cúng, xây chùa chiền ở nước sở tại,\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' nốc chunh uyên tở tạn chập nhạn, ron chúc sể ở bên ti ạn thỏ. tì']\n",
            "nếu được chính quyền sở tại chấp nhận, trong nước sẽ có hỗ trợ cụ thể. Thưa\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' ông nôy cốc đã tưc NHN bán khoán là tư các phýp thà củch d']\n",
            "ông, một vấn đề được nhiều NVƠNN băn khoăn là tư cách pháp nhân của bà\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' can lạ nớch tó trạ đó làn can cơ kã c thút vơ boác nậng']\n",
            "con tại nước sở tại để bà con có thể cư trú và làm ăn một\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' cánh chnh cáy . Tốn để nặy số cáa ra àm nọn thu ôyg. Côyg,ha']\n",
            "cách chính đáng. Vấn đề này sẽ được quan tâm như thế nào thưa ông? - Vâng, theo\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' trhnh khân củ tộn hánh tại, bý quạ gao bở ph tha mua v, lốn']\n",
            "tinh thần của Bộ Chính trị, Bộ Ngoại giao sẽ phải tham mưu và kiến\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' nhị n hĩ để đâm phan và tí biến Vớ cà mớc chung buy kụng']\n",
            "nghị cụ thể để đàm phán và ký kết với các nước những hiệp định\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' về ho tuy cánh phyếp tít nán công dân N N, vớ hị nhữ cá hậnh kung kình']\n",
            "về tư cách pháp lý của công dân VN, ví dụ như các hiệp định lãnh\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' n khậng tụnh họ mẹ. h thýp']\n",
            "sự, hiệp định hỗ trợ tư pháp\n",
            "(1, 1150, 32, 1)\n",
            "(1, 256, 216)\n",
            "[' Ngi N ra rhnh tống sẽ nớc ngài li nhâu hoàn ứnh khú nhân l']\n",
            "Người VN ra sinh sống ở nước ngoài từ nhiều hoàn cảnh khác nhau, do\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAAvCAYAAACi5cD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW5IAtOdGPQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fcaf20a9940>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "tL2B38Hl4QMA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img = test_set[0][0]['the_input'][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ypRIHWQqApyI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "ba9b7ee5-9980-4840-c7e3-ba3f1841fa49"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(np.squeeze(img).T)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fdfaf1d2160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAAvCAYAAACi5cD1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRORi/wEb4CGor3vS3wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdf97d32ac8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "JygLx8_WAssT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.misc\n",
        "scipy.misc.imsave('drive/My Drive/Japanese_Recognition/images/test_image.png', np.squeeze(img).T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WK-7FW5ZBKBw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "4f7d79a0-ce12-4580-cd0d-15c75ebf0031"
      },
      "cell_type": "code",
      "source": [
        "np.squeeze(img)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "metadata": {
        "id": "pbLLt1bNBTyH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}